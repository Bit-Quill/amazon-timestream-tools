{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6efcc07e",
   "metadata": {},
   "source": [
    "# Timestream Lambda Function Sample Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5f3c3",
   "metadata": {},
   "source": [
    "This notebook demonstrates generating data, according to a schema defined by the user; deploying an AWS Lambda function to process it; and visualizing the data using Grafana."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318886b7",
   "metadata": {},
   "source": [
    "## Step 1: Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da7d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def generate_random_string(length=6):\n",
    "    \"\"\"Generate a random alphanumeric string of a given length.\"\"\"\n",
    "    letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "    return ''.join(random.choice(letters) for _ in range(length))\n",
    "\n",
    "def generate_timestream_data(schema, num_records=10):\n",
    "    \"\"\"\n",
    "    Generate mock Timestream records with random dimension values and measures based on an input schema.\n",
    "    \n",
    "    Parameters:\n",
    "    - schema: A dictionary containing 'dimensions' and 'measures' lists.\n",
    "      Dimensions define 'DimensionName' and can optionally define a list of 'options' for random selection.\n",
    "      Measures define 'MeasureName', 'MeasureValueType', and optionally 'MeasureValueRange' or 'options'.\n",
    "      Example:\n",
    "      {\n",
    "          \"dimensions\": [\n",
    "              {\"DimensionName\": \"region\", \"options\": [\"us-west-2\", \"us-east-1\"]},\n",
    "              {\"DimensionName\": \"device_id\"}  # Will randomly generate a value if no options are given\n",
    "          ],\n",
    "          \"measures\": [\n",
    "              {\"MeasureName\": \"cost\", \"MeasureValueType\": \"DOUBLE\", \"MeasureValueRange\": [100.0, 500.0]},\n",
    "              {\"MeasureName\": \"energy_consumption\", \"MeasureValueType\": \"DOUBLE\", \"MeasureValueRange\": [50.0, 300.0]},\n",
    "              {\"MeasureName\": \"status\", \"MeasureValueType\": \"VARCHAR\", \"options\": [\"OK\", \"FAIL\"]}\n",
    "          ]\n",
    "      }\n",
    "    - num_records: Number of records to generate.\n",
    "\n",
    "    Returns:\n",
    "    - List of records suitable for ingestion into Timestream.\n",
    "    \"\"\"\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for _ in range(num_records):\n",
    "        record_time = str(int(time.time_ns()))  # Use a common nanosecond timestamp for all measures in the record\n",
    "\n",
    "        # Extract dimensions from the schema and generate random values if needed\n",
    "        record_dimensions = []\n",
    "        for dim in schema.get(\"dimensions\", []):\n",
    "            dimension_name = dim['DimensionName']\n",
    "            dimension_value = random.choice(dim.get(\"options\", [generate_random_string()]))\n",
    "            \n",
    "            record_dimensions.append({\n",
    "                \"Name\": dimension_name,\n",
    "                \"Value\": dimension_value\n",
    "            })\n",
    "\n",
    "        # Extract measures from the schema and generate random values\n",
    "        record_measures = []\n",
    "        for measure in schema.get(\"measures\", []):\n",
    "            measure_name = measure.get(\"MeasureName\")\n",
    "            measure_type = measure.get(\"MeasureValueType\")\n",
    "            \n",
    "            if measure_type == \"DOUBLE\":\n",
    "                measure_value = round(random.uniform(*measure.get(\"MeasureValueRange\", [0.0, 100.0])), 2)\n",
    "            elif measure_type == \"VARCHAR\":\n",
    "                measure_value = random.choice(measure.get(\"options\", [\"default\"]))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported MeasureValueType: {measure_type}\")\n",
    "\n",
    "            record_measures.append({\n",
    "                \"MeasureName\": measure_name,\n",
    "                \"MeasureValue\": str(measure_value),\n",
    "                \"MeasureValueType\": measure_type\n",
    "            })\n",
    "\n",
    "        # Construct the final record with multiple measures and dimensions\n",
    "        record = {\n",
    "            \"Dimensions\": record_dimensions,  # Use the generated dimensions\n",
    "            \"Time\": record_time,  # Same timestamp for all measures in this record\n",
    "            \"Measures\": record_measures  # Use the collected measures\n",
    "        }\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    return records\n",
    "\n",
    "# Example input schema with separated dimensions and measures\n",
    "schema = {\n",
    "    \"dimensions\": [\n",
    "        {\"DimensionName\": \"region\", \"options\": [\"us-west-2\", \"us-east-1\"]},\n",
    "        {\"DimensionName\": \"device_id\", \"options\": [\"fhncipr94k\", \"1o30plfurn\", \"1f49apz086\"]}  # Random string if no options are provided\n",
    "    ],\n",
    "    \"measures\": [\n",
    "        {\"MeasureName\": \"cost\", \"MeasureValueType\": \"DOUBLE\", \"MeasureValueRange\": [100.0, 500.0]},\n",
    "        {\"MeasureName\": \"energy_consumption\", \"MeasureValueType\": \"DOUBLE\", \"MeasureValueRange\": [50.0, 300.0]},\n",
    "        {\"MeasureName\": \"status\", \"MeasureValueType\": \"VARCHAR\", \"options\": [\"OK\", \"FAIL\"]}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Generate data using the new schema format\n",
    "sample_data = generate_timestream_data(schema, num_records=1000)\n",
    "\n",
    "# Print generated data\n",
    "print(json.dumps(sample_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a6cd5",
   "metadata": {},
   "source": [
    "## Step 2: Deploy AWS Lambda Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3368d6f6",
   "metadata": {},
   "source": [
    "The following code will construct and deploy a Lambda function that ingests data to Timestream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87489d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS SDK and Lambda deployment\n",
    "import boto3\n",
    "import zipfile\n",
    "import os\n",
    "import json\n",
    "\n",
    "REGION_NAME='us-west-2'\n",
    "DATABASE_NAME = 'sample_app_database'\n",
    "TABLE_NAME = 'sample_app_table'\n",
    "\n",
    "# Initialize clients\n",
    "\n",
    "iam_client = boto3.client('iam', region_name=REGION_NAME)\n",
    "lambda_client = boto3.client('lambda', region_name=REGION_NAME)\n",
    "sts_client = boto3.client('sts', region_name=REGION_NAME)\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "\n",
    "# Step 1: Create IAM Role for Lambda\n",
    "role_name = \"TimestreamLambdaRole\"\n",
    "assume_role_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\"Service\": \"lambda.amazonaws.com\"},\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "role_arn = \"\"\n",
    "\n",
    "try:\n",
    "    create_role_response = iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(assume_role_policy),\n",
    "        Description=\"Role for Lambda to write to Timestream\"\n",
    "    )\n",
    "    print(f\"Created IAM Role: {role_name}\")\n",
    "    role_arn = create_role_response['Role']['Arn']\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    print(f\"IAM Role {role_name} already exists\")\n",
    "    try:\n",
    "        role_arn = iam_client.get_role(RoleName=role_name)['Role']['Arn']\n",
    "    except iam_client.exceptions.NoSuchEntityException:\n",
    "        print(\"IAM Role could not be found\")\n",
    "        raise\n",
    "\n",
    "# CloudWatch logs policy to be added to the role\n",
    "cloudwatch_logs_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:CreateLogGroup\",\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": f\"arn:aws:logs:{REGION_NAME}:{account_id}:log-group:/aws/lambda/TimestreamIoTLambda*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Add the CloudWatch logs policy to the role\n",
    "try:\n",
    "    iam_client.put_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyName='CloudWatchLogsPolicy',\n",
    "        PolicyDocument=json.dumps(cloudwatch_logs_policy)\n",
    "    )\n",
    "    print(f\"Attached CloudWatch logs policy to role: {role_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error attaching CloudWatch logs policy: {e}\")\n",
    "\n",
    "# Step 2: Attach Policy to the IAM Role\n",
    "policy_arn = \"arn:aws:iam::aws:policy/AmazonTimestreamFullAccess\"\n",
    "iam_client.attach_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyArn=policy_arn\n",
    ")\n",
    "\n",
    "print(f\"Attached Timestream write policy to {role_name}\")\n",
    "\n",
    "# Step 3: Create Lambda Function Code\n",
    "lambda_function_code = '''\n",
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "REGION_NAME = os.environ['REGION_NAME']\n",
    "DATABASE_NAME = os.environ['DATABASE_NAME']\n",
    "TABLE_NAME = os.environ['TABLE_NAME']\n",
    "\n",
    "# Initialize the Timestream client\n",
    "timestream_client = boto3.client('timestream-write', REGION_NAME)\n",
    "\n",
    "# Define your table retention properties\n",
    "RETENTION_PROPERTIES = {\n",
    "    'MemoryStoreRetentionPeriodInHours': 24,  # Adjust as needed\n",
    "    'MagneticStoreRetentionPeriodInDays': 365  # Adjust as needed\n",
    "}\n",
    "\n",
    "def create_timestream_database_and_table():\n",
    "    \"\"\"\n",
    "    Create Timestream database and table if they do not exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create database if it does not exist\n",
    "        timestream_client.create_database(DatabaseName=DATABASE_NAME)\n",
    "        print(f\"Database '{DATABASE_NAME}' created successfully.\")\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'ConflictException':\n",
    "            print(f\"Database '{DATABASE_NAME}' already exists.\")\n",
    "        else:\n",
    "            raise e  # Raise if it's a different error\n",
    "\n",
    "    try:\n",
    "        # Create table if it does not exist\n",
    "        timestream_client.create_table(\n",
    "            DatabaseName=DATABASE_NAME,\n",
    "            TableName=TABLE_NAME,\n",
    "            RetentionProperties=RETENTION_PROPERTIES\n",
    "        )\n",
    "        print(f\"Table '{TABLE_NAME}' created successfully in database '{DATABASE_NAME}'.\")\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'ConflictException':\n",
    "            print(f\"Table '{TABLE_NAME}' already exists in database '{DATABASE_NAME}'.\")\n",
    "        else:\n",
    "            raise e  # Raise if it's a different error\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Lambda function to process the request and ingest records into Timestream.\n",
    "    The function accepts a list of records, handles MULTI measure types, \n",
    "    and sends data in batches of 100 records to Timestream.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the database and table if they do not exist\n",
    "    create_timestream_database_and_table()\n",
    "\n",
    "    try:\n",
    "        # Extract the records from the event\n",
    "        body = event.get('body', '{}')\n",
    "        parsed_body = json.loads(body)\n",
    "        records = parsed_body.get('records', [])\n",
    "        print(records)\n",
    "        if not records:\n",
    "            return {\n",
    "                \"statusCode\": 400,\n",
    "                \"body\": json.dumps(\"No records found in the request.\")\n",
    "            }\n",
    "\n",
    "        # Process records in batches of 100\n",
    "        for i in range(0, len(records), 100):\n",
    "            records_batch = records[i:i + 100]\n",
    "            # Prepare the records for Timestream\n",
    "            prepared_records = []\n",
    "\n",
    "            for record in records_batch:\n",
    "                dimensions = record.get(\"Dimensions\", [])\n",
    "                time_value = record.get(\"Time\")\n",
    "                measures = record.get(\"Measures\", [])\n",
    "\n",
    "                # Check if there are multiple measures, in which case we'll use MULTI\n",
    "                if len(measures) > 1:\n",
    "                    measure_value_type = 'MULTI'\n",
    "                    multi_value_measure = {\n",
    "                        'MeasureName': measures[0]['MeasureName'],  # Example MeasureName; Timestream expects just one for MULTI\n",
    "                        'MeasureValues': [\n",
    "                            {\n",
    "                                'Name': m['MeasureName'],\n",
    "                                'Value': m['MeasureValue'],\n",
    "                                'Type': m['MeasureValueType']\n",
    "                            }\n",
    "                            for m in measures\n",
    "                        ]\n",
    "                    }\n",
    "                    prepared_record = {\n",
    "                        'Dimensions': dimensions,\n",
    "                        'Time': time_value,\n",
    "                        'TimeUnit': 'NANOSECONDS',\n",
    "                        'MeasureName': multi_value_measure['MeasureName'],  # Set MULTI MeasureName\n",
    "                        'MeasureValueType': measure_value_type,\n",
    "                        'MeasureValues': multi_value_measure['MeasureValues']\n",
    "                    }\n",
    "                else:\n",
    "                    # Handle the case where there is only one measure\n",
    "                    measure = measures[0]\n",
    "                    prepared_record = {\n",
    "                        'Dimensions': dimensions,\n",
    "                        'Time': time_value,\n",
    "                        'TimeUnit': 'NANOSECONDS',\n",
    "                        'MeasureName': measure['MeasureName'],\n",
    "                        'MeasureValue': measure['MeasureValue'],\n",
    "                        'MeasureValueType': measure['MeasureValueType']\n",
    "                    }\n",
    "\n",
    "                prepared_records.append(prepared_record)\n",
    "\n",
    "            # Write to Timestream using the `write_records` API\n",
    "            response = timestream_client.write_records(\n",
    "                DatabaseName=DATABASE_NAME,\n",
    "                TableName=TABLE_NAME,\n",
    "                Records=prepared_records\n",
    "            )\n",
    "            print(f\"Batch write successful for records {i} to {i + len(records_batch) - 1}: {response}\")\n",
    "\n",
    "        return {\n",
    "            \"statusCode\": 200,\n",
    "            \"body\": json.dumps(f\"Successfully ingested {len(records)} records into Timestream.\")\n",
    "        }\n",
    "    \n",
    "    except ClientError as e:\n",
    "        print(f\"Failed to write to Timestream: {e}\")\n",
    "        return {\n",
    "            \"statusCode\": 500,\n",
    "            \"body\": json.dumps(f\"Error writing to Timestream: {str(e)}\")\n",
    "        }\n",
    "'''\n",
    "\n",
    "lambda_name = \"TimestreamIoTLambda\"\n",
    "\n",
    "# Save the Lambda function code to a file\n",
    "lambda_function_file = \"lambda_function.py\"\n",
    "with open(lambda_function_file, 'w') as f:\n",
    "    f.write(lambda_function_code)\n",
    "\n",
    "# Create a deployment package (zip file)\n",
    "lambda_zip = \"lambda_function.zip\"\n",
    "with zipfile.ZipFile(lambda_zip, 'w') as zipf:\n",
    "    zipf.write(lambda_function_file)\n",
    "\n",
    "# Step 4: Deploy Lambda Function\n",
    "lambda_name = \"TimestreamIoTLambda\"\n",
    "try:\n",
    "    with open(lambda_zip, 'rb') as f:\n",
    "        lambda_client.create_function(\n",
    "            FunctionName=lambda_name,\n",
    "            Runtime='python3.12',\n",
    "            Role=role_arn,\n",
    "            Handler='lambda_function.lambda_handler',\n",
    "            Architectures=['arm64'],\n",
    "            Code={'ZipFile': f.read()},\n",
    "            Environment={\n",
    "                'Variables': {\n",
    "                    'REGION_NAME': REGION_NAME,\n",
    "                    'DATABASE_NAME': DATABASE_NAME,\n",
    "                    'TABLE_NAME': TABLE_NAME\n",
    "                }\n",
    "            },\n",
    "            Timeout=30,\n",
    "            MemorySize=128\n",
    "        )\n",
    "    print(f\"Lambda function {lambda_name} created successfully.\")\n",
    "except lambda_client.exceptions.ResourceConflictException:\n",
    "    print(f\"Lambda function {lambda_name} already exists.\")\n",
    "\n",
    "# Clean up the files\n",
    "os.remove(lambda_function_file)\n",
    "os.remove(lambda_zip)\n",
    "\n",
    "# Step 4: Add a resource policy to allow invocation via the function URL\n",
    "\n",
    "try:\n",
    "    lambda_client.add_permission(\n",
    "        FunctionName=lambda_name,\n",
    "        StatementId='FunctionURLAllowInvoke',\n",
    "        Action='lambda:InvokeFunctionUrl',\n",
    "        Principal=role_arn,\n",
    "        FunctionUrlAuthType='AWS_IAM'\n",
    "    )\n",
    "    print(f\"Added resource policy to allow function URL invocation for {lambda_name}.\")\n",
    "except lambda_client.exceptions.ResourceConflictException:\n",
    "    print(f\"Resource policy for {lambda_name} already exists.\")\n",
    "\n",
    "# Create or get the Lambda Function URL\n",
    "try:\n",
    "    response = lambda_client.create_function_url_config(\n",
    "        FunctionName=lambda_name,\n",
    "        AuthType='AWS_IAM'\n",
    "    )\n",
    "    function_url = response['FunctionUrl']\n",
    "    print(f\"Lambda Function URL: {function_url}\")\n",
    "except lambda_client.exceptions.ResourceConflictException:\n",
    "    # If the URL configuration already exists, retrieve it\n",
    "    response = lambda_client.get_function_url_config(FunctionName=lambda_name)\n",
    "    function_url = response['FunctionUrl']\n",
    "    print(f\"Lambda Function URL (existing): {function_url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef5a36c",
   "metadata": {},
   "source": [
    "## Step 3: Send Data to the Lambda Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1066096a",
   "metadata": {},
   "source": [
    "The following code will send the generated sample data to the Lambda function's URL with SigV4 authenticated requests, ensuring requests do not exceed Lambda's limit of 6 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08f884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import boto3\n",
    "from botocore.auth import SigV4Auth\n",
    "from botocore.awsrequest import AWSRequest\n",
    "\n",
    "MAX_REQUEST_SIZE = 6 * 1024 * 1024  # 6 MB in bytes\n",
    "\n",
    "def send_data_to_lambda(data):\n",
    "    \"\"\"Sends generated data to the Lambda function in chunks.\"\"\"\n",
    "    service = 'lambda'\n",
    "    region = REGION_NAME\n",
    "    method = \"POST\"\n",
    "\n",
    "    session = boto3.Session(region_name=region)\n",
    "\n",
    "    # Calculate the size of the entire data payload\n",
    "    data_payload = json.dumps({'records': data})\n",
    "    total_size = len(data_payload.encode('utf-8'))\n",
    "\n",
    "    # Check if the total size exceeds the maximum request size\n",
    "    if total_size <= MAX_REQUEST_SIZE:\n",
    "        send_request(session, method, data_payload)\n",
    "    else:\n",
    "        # Chunk the data if it's too large\n",
    "        chunk_size = MAX_REQUEST_SIZE - len(b'{\"records\":[]}')  # Reserve space for the JSON structure\n",
    "        chunks = [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "        for chunk in chunks:\n",
    "            chunk_payload = json.dumps({'records': chunk})\n",
    "            send_request(session, method, chunk_payload)\n",
    "\n",
    "def send_request(session, method, payload):\n",
    "    \"\"\"Sends the request to the Lambda function.\"\"\"\n",
    "    request = AWSRequest(\n",
    "        method=method,\n",
    "        url=function_url,\n",
    "        headers={'Content-Type': 'application/json'},\n",
    "        data=payload\n",
    "    )\n",
    "\n",
    "    SigV4Auth(session.get_credentials(), 'lambda', REGION_NAME).add_auth(request)\n",
    "\n",
    "    try:\n",
    "        response = requests.request(method, function_url, headers=dict(request.headers), data=payload, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        print(f'Response Status: {response.status_code}')\n",
    "        print(f'Response Body: {response.content.decode(\"utf-8\")}')\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}')\n",
    "\n",
    "# Send sample data to the Lambda function\n",
    "send_data_to_lambda(sample_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc25f96b",
   "metadata": {},
   "source": [
    "## Step 4: Configure Grafana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112cf6a6",
   "metadata": {},
   "source": [
    "### Grafana Configuration Steps\n",
    "1. Open Grafana.\n",
    "2. Add a new data source with the name \"timestream_sample_app_database\".\n",
    "3. Create a new dashboard using the provided `dashboard.json`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
